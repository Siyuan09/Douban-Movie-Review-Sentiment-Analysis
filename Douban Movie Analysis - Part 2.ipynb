{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048575 entries, 0 to 1048574\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   ID             1048575 non-null  int64 \n",
      " 1   Movie_Name_EN  1048575 non-null  object\n",
      " 2   Movie_Name_CN  1048575 non-null  object\n",
      " 3   Crawl_Date     1048575 non-null  object\n",
      " 4   Number         1048575 non-null  int64 \n",
      " 5   Username       1048505 non-null  object\n",
      " 6   Date           1048575 non-null  object\n",
      " 7   Star           1048575 non-null  int64 \n",
      " 8   Comment        1048575 non-null  object\n",
      " 9   Like           1048575 non-null  int64 \n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 80.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# load movie reviews dataset\n",
    "data = pd.read_csv('DMSC.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label(star):\n",
    "    if star > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['Sentiment'] = data.Star.apply(make_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/m3/fy9sk0m56rz89mjqdd9_9chh0000gn/T/jieba.cache\n",
      "Loading model cost 0.826 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "data['Cut_Comment'] = data.Comment.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "\n",
    "stop_words_file = 'stop_words_chinese.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vect = CountVectorizer(max_df = 0.8, \n",
    "                       min_df = 3, \n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b', \n",
    "                       stop_words=frozenset(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Cut_Comment']\n",
    "y = data.Sentiment\n",
    "\n",
    "def evaluate_model(X,Y,model, model_name, params):\n",
    "    #Initialize the seed value to 1\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #Split the data to test and train data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    X_train_vect = vect.fit_transform(X_train)\n",
    "    X_test_vect = vect.transform(X_test)\n",
    "   \n",
    "    #hyper-parameter tuningï¼Œusing model and input params to choose the best model\n",
    "    clf = GridSearchCV(model, params, cv=10)\n",
    "    \n",
    "    #Evaluate the test error using the best classifier and the test data\n",
    "    clf.fit(X_train_vect, y_train)\n",
    "    Score = clf.score(X_test_vect, y_test)\n",
    "    Macro_F1_Score = f1_score(y_test, clf.predict(X_test_vect), average='macro')\n",
    "    Micro_F1_Score = f1_score(y_test, clf.predict(X_test_vect), average='micro')\n",
    "    CV_Score = cvres['mean_test_score'][clf.best_index_]  \n",
    "    \n",
    "    #9.6 return a dictionary\n",
    "    d = {'Classifier': model_name, 'params':clf.best_estimator_,'Test Score': Score, \\\n",
    "         'CV Score':CV_Score, 'Macro F1 Score':Macro_F1_Score, 'Micro F1 Score':Micro_F1_Score}\n",
    "    return d\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_classifiers():\n",
    "    return([(SVC(), model_names[0], param_grid_svc), \n",
    "            (LogisticRegression(), model_names[1], param_grid_logistic),\n",
    "            (KNeighborsClassifier(), model_names[2], param_grid_knn),\n",
    "            (MultinomialNB(), model_names[3], param_grid_nb),\n",
    "            (DecisionTreeClassifier(), model_names[4], param_grid_tree),\n",
    "            (RandomForestClassifier(), model_names[5], param_grid_rf),\n",
    "            (AdaBoostClassifier(), model_names[6], param_grid_adaboost),\n",
    "            (MLPClassifier(),model_names[7],param_grid_MLP)])\n",
    "\n",
    "# 'model_names' contains the names  that we will use for the above classifiers\n",
    "model_names = ['SVM','LR','KNN','NB','DecisionTree','RF','AdaBoost','MLP']\n",
    "\n",
    "# the training parameters of each model\n",
    "param_grid_svc = [{'C':[0.1,1],'kernel':['rbf','linear','poly','sigmoid'],'random_state':[1]}]\n",
    "param_grid_logistic = [{'C':[0.1,1], 'penalty':['l1','l2'],'random_state':[1]}]\n",
    "param_grid_knn = [{'n_neighbors':list(range(1,31))}]\n",
    "param_grid_nb = [{}]\n",
    "param_grid_tree = [{'random_state':[1]},{'criterion':['gini'], 'max_depth':list(range(10,100)), \\\n",
    "                                         'min_samples_split':[3,5],'random_state':[1]}]\n",
    "param_grid_rf = [{'random_state':[1]},{'n_estimators':[50,70,100,150],'max_features':[0.2, 0.3], \\\n",
    "                                       'max_depth':list(range(10,100)),'bootstrap':[True],'random_state':[1]}]\n",
    "param_grid_adaboost = [{'random_state':[1]},{'n_estimators':[50,70,100,150],'learning_rate':[0.1,1],\\\n",
    "                                             'random_state':[1]}]\n",
    "param_grid_MLP = [{'hidden_layer_sizes':[100,200],'solver':['lbfgs', 'sgd', 'adam'],'random_state':[1],\\\n",
    "                   'activation':['identity', 'logistic', 'tanh', 'relu'],\\\n",
    "                   'learning_rate':['constant', 'invscaling', 'adaptive'],'alpha':list(range(0,1))}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "classifiers = init_classifiers()\n",
    "for i in classifiers:\n",
    "    results = evaluate_model(X, y, i[0], i[1], i[2])\n",
    "    res_list.append(results)\n",
    "\n",
    "df_model_comparison = pd.DataFrame(res_list).sort_values(['Classifier']).reset_index(drop=True)\n",
    "df_model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
